% status: 100
% chapter: Synthetic Data Vault

\title{Synthetic Data Vault}

\author{Tim Whitson}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Research Center}
  \city{Bloomington}
  \state{IN}
  \postcode{47408}
  \country{USA}
}
\email{tdwhitso@indiana.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{T. Whitson}

\begin{abstract}
Data is the key tool of insight for Data Scientists, so access to
actionable data is paramount. However, for many reasons, access to data is
not guaranteed. Synthetic data solves the problems of a lack of data or a
lack of access to data. Synthetic Data Vault (SDV), which was developed in
2016 at MIT, is the premier technology in synthetic data generation. Here,
we explore SDV --- why it is necessary, how it generates data, and how to
use the synthetic data.
\end{abstract}

\keywords{hid-sp18-526, machine learning, synthetic data}

\maketitle

\section{Introduction}

Data analysis has changed the landscape of business and research. All data
has a story, and data scientists are tasked with telling that story. However,
what if the data is inaccessible to the data scientists? Are they out of
luck or is it possible to create data that is still useful?

The solution to this problem is synthetic data. Synthetic data can be generated
to look (and act) like real data. In the same way that predictions can be
made using existing data, synthetic data can be generated. While it may seem
illogical, synthetic data can also be analyzed and used to train models.

The most notable synthetic data generator is the Synthetic Data Vault
(SDV). Developed at MIT by Neha Patki, Roy Wedge, and Kalyan Veeramachaneni,
the Synthetic Data Vault uses machine learning techniques to model database
structure and content. The models can then be used to generate entirely
synthetic tables and relationships which are true to the form of the
originals. Because the synthetic data is generated and modeled mathematically
according to the original data, very little, if any, insight is lost. Here
we will explore why we should use synthetic data, how SDV generates synthetic
data, and how to use the data generated by SDV.\cite{hid-sp18-526-patki-sdv}

\subsection{Requirements}

SDV is written in Python and is, therefore, cross-platform. A separate file
is required for each database in the table. Also, each database requires a
configuration file in json format. The specifications for the configuration
file will be shown later, but first we will discuss why such a product is
necessary.\cite{hid-sp18-526-www-sdv-github}

\section{Why Synthetic Data Vault?}

Data science and analysis has driven industry to new areas never before
explored. One look at Kaggle or DrivenData shows the usefulness and value
that data provides organizations who nourish it. However, organizations
also face hurdles with their data (or lack thereof). Consider healthcare
companies or defense contractors with highly sensitive data that cannot
be shared publicly. Also consider small businesses who don't possess the
knowledge or manpower to collect real data. These organizations need a
solution to the lack of access to data.

Assuming that synthetic data is as insightful as real data, these
organizations would all benefit from synthetic data generated by SDV. In
the seminal publication for SDV, Neha Patki outlines two motivations for
using synthetic data: ``Populating Sample Databases'' and ``Scaling Data
Science Efforts''\cite{hid-sp18-526-patki-sdv}. Consider, for example, a company using
a Kaggle or DrivenData competition to gain insight into their data. The
company cannot release real data because it would expose their users and
create privacy concerns. They must find a way to release data publicly in
order for a proper analysis to be done. They can generate such data using
SDV. This is one example amongst many where synthetic data solves the issue
of data availability (or scaled efforts, according to Patki, as more data
scientists can now access the data).

\subsection{Privacy}

Whether for business reasons or privacy issues, most data is sensitive. Any
attempt at a public display of data could lead to a loss of revenue, lawsuits,
damage to national security, etc. Synthetic data adheres to the structure of
the original data without any required real-world underpinning. So patient
data, for example, could be generated and used by businesses and researchers
to improve patient outcome without any of the pitfalls or added steps of
using real patient data. With the modern internet landscape, user privacy
is taken very seriously. SDV provides a solution to keeping data private.

\section{Generating Synthetic Data}

The first step in synthetic data analysis is to generate the data. SDV
accomplishes this task using machine learning techniques. First, the original
data is analyzed and modeled. Then, the models are used to generate the
synthetic data. The benefit of machine learning is that the models are able
to recreate realistic synthetic data points based on statistical analysis
of the data.\cite{hid-sp18-526-patki-wedge-veer-sdv}

Most data that would be synthetically modeled sits in a database. Therefore,
SDV works specifically with tables inside databases. By analyzing each column,
or feature, of the data, SDV can accurately generate unique data that conforms
to the original. Each numerical row, for example, has a min, max, mean, etc.\
which will continue to be followed upon synthesis.\cite{hid-sp18-526-patki-wedge-veer-sdv}

The relationship between the tables is analyzed as well. For this task,
the tables must show a many-to-one relationship. Many-to-many tables must
be broken down into many-to-one. SDV detects if an ID is being used as a
foreign key and maps the relationship.\cite{hid-sp18-526-patki-sdv}

Take, for example, a database filled with hospital patient data. Due to
the private nature of health data, analysts might not have access to the
data. However, a synthetic recreation of the data is possible, which would be
perfectly usable for analysis. Suppose there are fields for ``height'' and
``weight''. These features have a relationship, positive correlation, which
would be captured by the machine learning algorithm. Also suppose height is
normally distributed. Combine all of these factors together and a normal
distribution of heights could be generated with matching weights. While
this example is oversimplified, it shows that synthetically-generated data
is far more than just random data.\cite{hid-sp18-526-patki-wedge-veer-sdv}

Patki et al.\ define four steps to the SDV\cite{hid-sp18-526-patki-wedge-veer-sdv}:
\begin{enumerate}
    \item Organize
    \item Specify Structure
    \item Learn Model
    \item Synthesize Data
\end{enumerate}

\subsection{Organize}

Two arguments are required for the modeling to begin: the configuration file
and the database file. Therefore, the database must first be separated by
table into individual files.\cite{hid-sp18-526-patki-wedge-veer-sdv}

\subsection{Specify Structure}

Next, It is necessary to provide metadata to the program. Due to the
complexity and variety of data, SDV is not capable of deriving datatypes
automatically. The user must specify a type for each column. ID columns, for
example, must be identified separately. They are crucial to the generation
of new data because they often times are used as foreign keys in other
tables. Any other columns requiring special consideration also need to be
specified, such as datetime. It is possible to specify custom schema by
supplying a regular expression for a column. Patki describes five different
data types\cite{hid-sp18-526-patki-wedge-veer-sdv}:
\begin{enumerate}
    \item Number: A numerical value, either an integer or decimal.
    \item Categorical: Discrete categories. These can be represented as text
    or numbers.
    \item Datetime: Time information, with a specified format.
    \item ID: Either identifying information for the table rows, or references
    to other
tables in the database. These could be represented as numerical values or as
text.
    \item Text: Raw text that should not be modeled. If there is any structure
    to the
text, the user can provide a regex describing it.
\end{enumerate}

\subsection{Learn Model}

Next, SDV ``learns'' the database. Here, the schema is created. Each table is
modeled along with its relationship to other tables, which the authors call
Conditional Parameter Aggregation (CPA). With this step, SDV can accurately
recreate not only the information within the database, but its structure as
well. An analysis of the data is now done. SDV checks the distribution of the
data to determine if it is uniform, exponential, etc.\ Then covariances between
the data features is calculated. This model is stored so that it can be used
at any point of time in the future for synthesis.\cite{hid-sp18-526-patki-wedge-veer-sdv}

Missing data is handled specially by SDV. Two new columns are generated
upon finding missing data. The first is a column with the missing data
filled in randomly. The other is a boolean column declaring whether or not
missing data was filled in. Missing data is obviously not a problem upon
synthesis.\cite{hid-sp18-526-patki-wedge-veer-sdv}

\subsection{Synthesize Data}

Finally, the data is synthesized. Tables can be synthesized and their child
(dependent) tables can be synthesized as well. Here, the user also has
the option of viewing the model of the database. The generated tables can
immediately be used in place of the originals.\cite{hid-sp18-526-patki-wedge-veer-sdv}

\section{Use of Synthetic Data}

The use of synthetic data goes beyond privacy concerns. As Patki stated, one
use of the SDV is for ``Populating Sample Databases''\cite{hid-sp18-526-patki-sdv}. Data
scientists can generate as much, or as little, data as is necessary. So,
if a Data Scientist needs to test scaling or do stress tests, he/she can
continue generating larger amounts of data.

The flexibility of SDV is also a great advantage. Instead of requiring
storage of, or access to, an entire database, all data scientists need are
the models. From those models, they can generate whatever they need to work
with. For example, a remote server with a database could be accessed and
modeled without ever requiring entrance into the actual database.

\subsection{Accuracy}

Synthetic data would not be very useful if it could not take the place of
real data during analysis. In the sample study behind the project, Patki et
al.\ gave both real and synthesized data to data scientists to run analysis
on. The results, albeit with a small sample, show no statistically significant
difference between real and synthesized data in predictive modeling. Therefore,
it can be tentatively concluded that synthetic data can be used for analysis
in lieu of real data.\cite{hid-sp18-526-patki-wedge-veer-sdv}

\subsection{Limitations}

Of course, SDV has its limitations. Sometimes data just may not be able to
be recreated easily. For example, SDV currently only tests for Gaussian,
beta, exponential and uniform distributions. Any other distribution is
unsupported.\cite{hid-sp18-526-patki-wedge-veer-sdv}

It must also be stated that synthetic data cannot replace natural
data entirely. Firstly, the models for the data are created using real
data. Relying solely on data generated from an unrelated dataset would
be disastrous in predictions. In her article, ``At a Glance: Generative
Models \& Synthetic Data'', Cassie Sanchez points out ``No model will ever
be able to generate examples of things it's never seen real examples of
before.''\cite{hid-sp18-526-www-sanchez-glance}

\section{Conclusion}

Data Scientists face many hurdles before they can begin their analysis,
including inaccessible data. Without access to the data, no analysis can
be done. SDV solves this problem and is on the forefront of synthetic data
generation. Using SDV, organizations can easily model and generate data on
command that stays syntactically true to the original. Whether organizations
want to do analysis on healthcare patients or create a Kaggle competition
for new insights --- SDV is the tool for the job.

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

